import os, shutil
import torch
import gc
import psutil
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFacePipeline
from langchain_ollama import OllamaLLM

# === 0) Chemins locaux ===
MERGED_MODEL_PATH = r".\Phi4_merged"

# Prompt systeme
system_prompt = """
Tu es un expert en virologie et infectiologie sp√©cialis√© dans la prise en charge du VIH/SIDA.
√Ä partir des mutations g√©n√©tiques, des scores d'efficacit√© des antiretroviraux (ARV), et du contexte clinique du patient, g√©n√®re une interpr√©tation clinique d√©taill√©e de 150 mots du g√©notype, sous forme de rapport m√©dical structur√©.
Ton analyse doit comprendre :
1. Un **cadre virologique** expliquant les √©checs th√©rapeutiques pass√©s et les mutations majeures en jeu.
2. Une **analyse des r√©sistances crois√©es** et des m√©dicaments encore efficaces.
3. Une **proposition de traitement optimis√©** selon les ARV disponibles et une proposition d'association de molecules de la forme molecule1+molecule2+molecule3.
4. Un **commentaire sur l'observance** et le **suivi virologique √† pr√©voir**.
Utilise un vocabulaire m√©dical pr√©cis, sans faute, et emploie un ton professionnel.
R√©dige ta reponse en seul paragraphe.

Ne rep√®te pas sous aucun pr√©texte le prompt dans la r√©ponse que tu vas g√©n√©rer.
"""

# Format de r√©ponse attendue
calque = """
Cadre g√©notypique compatible √† un d√©but d‚Äô√©chec √† toutes les mol√©cules du traitement antir√©troviral en cours, caract√©ris√© sp√©cifiquement par la pr√©sence de la mutation M46MI associ√©e √† la r√©sistance de faible degr√© aux IP/r (ATV/r et LPV/r) ; 
et des mutations associ√©es √† la r√©sistance de degr√©s variables aux INTIs et de degr√© √©lev√© aux INNTIs de premi√®re g√©n√©ration (EFV et NVP) y compris DOR. Toutefois, l‚ÄôAZT garde une bonne efficacit√© r√©siduelle et et TDF une efficacit√© partielle 
(tous deux favoris√©s par la mutation M184V). Par ailleurs, prenant en compte l‚Äô√©volution lente du virus (une seule mutation majeure additionnelle par rapport au g√©notype de 2000), ainsi que l‚Äôexposition relativement courte sous TLD sugg√®re 
une efficacit√© pr√©serv√©e du DTG, sous r√©serve de la confirmation par s√©quen√ßage de la r√©gion d‚Äôint√©grase.

Au regard de l‚Äôefficacit√© totale du DRV/r, de la bonne efficacit√© r√©siduelle de l‚ÄôATV/r, du LPV/r et de l‚ÄôAZT, et de l‚Äôefficacit√© partielle du TDF et ABC, dans un contexte de vir√©mie mod√©r√©e (de l‚Äôordre de 3 Log), un changement rapide de la th√©rapie est n√©cessaire, 
prenant en compte TDF*+3TC+DTG (pr√©f√©rentiellement) ou TDF*+3TC+DRV/r (alternativement).

* En cas d‚Äôusage de l‚ÄôAZT (mol√©cule ayant une bonne efficacit√© r√©siduelle contre le VIH), la pr√©sence du TDF reste indispensable dans la prise en charge du patient (pour le traitement de l‚Äôh√©patite B). 

NB : Bien qu‚Äôinefficace, la pr√©sence du 3TC permet de maintenir la mutation M184V qui √† son tour favorise l‚Äôefficacit√© du TDF tout en inhibant la r√©plication virale. 

Insister sur l‚Äôimportance de l‚Äôobservance pour la r√©ussite du traitement, √©valuer la vir√©mie plasmatique √† 12 semaines pour s‚Äôassurer de l‚Äôatteinte d‚Äôune charge virale ind√©tectable, et ensuite continuer le suivi virologique annuel selon les directives nationales. 
Une actualisation des taux de lymphocytes T CD4 serait b√©n√©fique pour la pr√©vention/surveillance des risques d‚Äô√©mergence des maladies opportunistes. 

"""

# M√©moire
MEMORY_FILE = r".\memoire.txt"
if not os.path.exists(MEMORY_FILE):
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        f.write("")

# === 1) Device & dtype ===
device = "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")
dtype = torch.bfloat16 if device == "cuda" and torch.cuda.is_bf16_supported() else torch.float16 if device == "cuda" else torch.float32
print(f"[INFO] device={device} | dtype={dtype}")

def cleanup_memory():
    """Nettoyer la m√©moire de fa√ßon s√©curis√©e"""
    gc.collect()
    if device == "cuda":
        try:
            torch.cuda.empty_cache()
            print("‚úÖ M√©moire CUDA nettoy√©e")
        except RuntimeError as e:
            print(f"‚ö†Ô∏è Impossible de vider le cache CUDA: {e}")
    elif device == "mps":
        try:
            torch.mps.empty_cache()
            print("‚úÖ M√©moire MPS nettoy√©e")
        except:
            print("‚ö†Ô∏è Impossible de vider le cache MPS")

def liberer_ram():
    """Lib√®re la m√©moire RAM avant une op√©ration critique"""
    process = psutil.Process(os.getpid())
    mem_avant = process.memory_info().rss / 1024 / 1024  # MB
    
    # Force le garbage collection
    import gc
    gc.collect()
    
    # Si CUDA est disponible
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
        except:
            pass
    
    mem_apres = process.memory_info().rss / 1024 / 1024
    print(f"üìä M√©moire lib√©r√©e: {mem_avant - mem_apres:.1f} MB")

cleanup_memory()

# === 2) Tokenizer ===
try:
    tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else "<|endoftext|>"
    print("‚úÖ Tokenizer charg√© avec succ√®s")
except Exception as e:
    print(f"‚ùå Erreur lors du chargement du tokenizer: {e}")
    # Fallback
    from transformers import GPT2Tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token

# === 3) Charger le mod√®le avec meilleure gestion d'erreurs ===
offload_dir = r".\phi4_offload"
shutil.rmtree(offload_dir, ignore_errors=True)
os.makedirs(offload_dir, exist_ok=True)

model = None
try:
    # Essayer sans quantification mais avec gestion m√©moire
    print("üîÑ Tentative sans quantification...")
    model = AutoModelForCausalLM.from_pretrained(
        MERGED_MODEL_PATH,
        device_map="auto",
        torch_dtype=dtype,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    model.eval()
    print("‚úÖ Mod√®le charg√© sans quantification")
    
except Exception as e:
    print(f"‚ùå Erreur sans quantification: {e}")
    # Fallback: CPU seulement avec float32
    print("üîÑ Tentative sur CPU...")
    model = AutoModelForCausalLM.from_pretrained(
        MERGED_MODEL_PATH,
        device_map="cpu",
        torch_dtype=torch.float32,
        trust_remote_code=True
    )
    model.eval()
    print("‚úÖ Mod√®le charg√© sur CPU")

# === 4) Pipeline HuggingFace avec param√®tres optimis√©s ===
try:
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=812,  # R√©duit pour √©conomiser la m√©moire
        temperature=0.2,
        do_sample=True,
        device_map="auto",
        batch_size=1,  # Important pour √©viter les probl√®mes m√©moire
        pad_token_id=tokenizer.eos_token_id
    )
    hf_llm = HuggingFacePipeline(pipeline=pipe)
    print("‚úÖ Pipeline cr√©√© avec succ√®s")
except Exception as e:
    print(f"‚ùå Erreur lors de la cr√©ation du pipeline: {e}")
    hf_llm = None

# Templates et chaines (inchang√©s)
template = """Syst√®me : {system_prompt}
Utilisateur : {user_prompt}
Utilise le m√™me vocabulaire que le {calque} mais en l'adaptant aux donn√©es du patient
R√©ponse :
"""

prompt = ChatPromptTemplate.from_template(template)

if hf_llm:
    chain = prompt | hf_llm
    print("‚úÖ Mod√®le fine-tun√© charg√© avec succ√®s")
else:
    print("erreur")

# === 5) M√©moire conversationnelle ===
def read_memory():
    """Lit le contenu du fichier m√©moire"""
    if not os.path.exists(MEMORY_FILE):
        return ""
    with open(MEMORY_FILE, "r", encoding="utf-8") as f:
        return f.read()

def append_memory(user_msg, model_resp):
    """Ajoute l'interaction au fichier m√©moire"""
    with open(MEMORY_FILE, "a", encoding="utf-8") as f:
        f.write(f"\nUSER: {user_msg}\nMODEL: {model_resp}\n")

# === 6) G√©n√©ration avec meilleure gestion m√©moire ===
def generate_model_response(user_msg: str):
    try:
        cleanup_memory()
        
        memory_context = read_memory()
        user_prompt_with_memory = f"{memory_context}\n{user_msg}"
        
        result = chain.invoke({
            "system_prompt": system_prompt, 
            "user_prompt": user_prompt_with_memory,
            "calque": calque
        })
        
        reponse = result.content if hasattr(result, "content") else str(result)
        final_form = reponse.split("R√©ponse :")[-1].strip()
        append_memory(user_msg, final_form)
        
        cleanup_memory()
        return final_form
        
    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration de r√©ponse : {e}")
        cleanup_memory()
        return "Je suis d√©sol√©, il y a eu une erreur."


